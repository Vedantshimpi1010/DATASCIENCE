{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":262482,"sourceType":"datasetVersion","datasetId":109852}],"dockerImageVersionId":29994,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Rossman Store Sales Prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Steps: \n1. Explatory Data Analysis\n2. Time Series Analysis \n    2.1. Predictive Modeling \n3. Results","metadata":{}},{"cell_type":"markdown","source":"![](https://m.strelapark.de/fileadmin/_processed_/csm_rossmann_shop_foto_stralsund_1633a5fb67.jpg)","metadata":{}},{"cell_type":"markdown","source":"## Used dataset is **rossmann store data**. It operates over 3,000 drug stores in 7 European countries. The challenge is to predict their daily sales for up to six weeks in advance.","metadata":{}},{"cell_type":"code","source":"pip install --upgrade pip'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing required libraries\nimport numpy as np\nimport pandas as pd, datetime\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\nfrom time import time\nimport os\nfrom math import sqrt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport itertools\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import acf,pacf\nfrom statsmodels.tsa.arima_model import  ARIMA\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom pandas import DataFrame\nimport xgboost as xgb\nfrom fbprophet import Prophet\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import datast \nstore = pd.read_csv('../input/rossmann-store-sales/store.csv')\ntrain = pd.read_csv('../input/rossmann-store-sales/train.csv', index_col='Date', parse_dates=True)\ntest = pd.read_csv('../input/rossmann-store-sales/test.csv')\ntrain.shape, test.shape, store.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Explamatory Data Analysis(EDA)**","metadata":{}},{"cell_type":"markdown","source":"### 1.1: Trends & Seasonility \nHow the sales vary with month, promo(First promotional Offer), promo2(Second Promotional Offer) and years. ","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train data as almost 1M observations of sales data over the year of appriximatelly (2013-2015). \nOkay, bread Date column in Year, Month, Day, Week columns","metadata":{}},{"cell_type":"code","source":"# Extract Year, Month, Day, Wee columns \ntrain['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Day'] = train.index.day\ntrain['WeekofYear'] = train.index.weekofyear\n\ntrain['SalesPerCustomer'] = train['Sales']/train['Customers']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the data when the store is closed \ntrain_store_closed = train[(train.Open == 0)]\ntrain_store_closed.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check when the store was closed \ntrain_store_closed.hist('DayOfWeek')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this chart, we could see that, 7th day store was mostly clodes. It is Sunday and makes sense. ","metadata":{}},{"cell_type":"code","source":"# Check whether there school was closed for holyday \ntrain_store_closed['SchoolHoliday'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Here 1 is school closed day and it pretty low. And 0 is None. ","metadata":{}},{"cell_type":"code","source":"# Check whether there school was closed for holyday \ntrain_store_closed['StateHoliday'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, The state is closed for (a= Public holyday, b = Easter holyday, c = Christmas and 0 is None)","metadata":{}},{"cell_type":"code","source":"# Check the null values\n# In here there is no null value \ntrain.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of days with closed stores\ntrain[(train.Open == 0)].shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Okay now check No. of dayes store open but sales zero ( It might be caused by external refurbishmnent)\ntrain[(train.Open == 1) & (train.Sales == 0)].shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Work with store data \nstore.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check null values \n# Most of the columns has null values \n\nstore.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replacing missing values for Competiton distance with median\nstore['CompetitionDistance'].fillna(store['CompetitionDistance'].median(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No info about other columns - so replcae by 0\nstore.fillna(0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again check it and now its okay \n\nstore.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Work with test data \ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check null values ( Only one feature Open is empty)\ntest.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming stores open in test\ntest.fillna(1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again check \ntest.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Join train and store table \ntrain_store_joined = pd.merge(train, store, on='Store', how='inner')\ntrain_store_joined.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_store_joined.groupby('StoreType')['Customers', 'Sales', 'SalesPerCustomer'].sum().sort_values('Sales', ascending='desc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Closed and zero-sales observations \ntrain_store_joined[(train_store_joined.Open == 0) | (train_store_joined.Sales==0)].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we have 172,871 observations when the stores were closed or have zero sales.","metadata":{}},{"cell_type":"code","source":"# Open & Sales >0 stores\ntrain_store_joined_open = train_store_joined[~((train_store_joined.Open ==0) | (train_store_joined.Sales==0))]\ntrain_store_joined_open","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.heatmap(train_store_joined.corr(), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above chart we can see a strong positive correlation between the amount of Sales and Customers visiting the store. We can also observe a positive correlation between a running promotion (Promo = 1) and number of customers.","metadata":{}},{"cell_type":"code","source":"# Now plot the sales trend over the month \nsns.factorplot(data = train_store_joined_open, x='Month', y='Sales',\n              col ='Promo', hue='Promo2', row='Year')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sales and trend over days\nsns.factorplot(data= train_store_joined_open, x='DayOfWeek', y=\"Sales\",\n              hue='Promo')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From the above chart, 0 represents sales and 1 represents promotin in a week. Promotions are not given in weekend (Saturday and Sunday). Because peoples are goinf to buy their household things on the weekend and wothout promotion sales increased in a dramatic way. Promotion are highest on monday and as well as sales are high on that day. ","metadata":{}},{"cell_type":"markdown","source":"# **Insights**\n### 1. Storetype a has highest customer and sales \n### 2. Storetype b has highest SalesPerCustomer \n### 3. There is no promotion offer in Saturday and Sunday\n### 4. Customers are going to buy their goods in tuesday on promotional offer. ","metadata":{}},{"cell_type":"markdown","source":"# 2. Time Series Analysis ","metadata":{}},{"cell_type":"markdown","source":"In this section we will consider only one store from each store type(a, b, c, d). ","metadata":{}},{"cell_type":"code","source":"pd.plotting.register_matplotlib_converters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Register pandas formatters and converters with matplotlib.\n\nThis function modifies the global matplotlib.units.registry dictionary. pandas adds custom converters for\n\npd.Timestamp\n\npd.Period\n\nnp.datetime64\n\ndatetime.datetime\n\ndatetime.date\n\ndatetime.time","metadata":{}},{"cell_type":"code","source":"# Data Preparation: input should be float type \n\n# our Sales data is int type so lets make it float\ntrain['Sales'] = train['Sales'] * 1.00\n\ntrain['Sales'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.Store.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning one store from each category\nsales_a = train[train.Store == 2]['Sales']\nsales_b = train[train.Store == 85]['Sales'].sort_index(ascending = True) \nsales_c = train[train.Store == 1]['Sales']\nsales_d = train[train.Store == 13]['Sales']\n\nframe, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize = (20, 16))\n\n# Visualize Trend \nsales_a.resample('w').sum().plot(ax = ax1)\nsales_b.resample('w').sum().plot(ax = ax2)\nsales_c.resample('w').sum().plot(ax = ax3)\nsales_d.resample('w').sum().plot(ax = ax4)\n\n\n# will be used to resample the speed column of our DataFrame\n#The 'W' indicates we want to resample by week. At the bottom of this post is a summary of different time frames.\n# You could use for Day = d, MOnth = m, Year = y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above chart we could see sales of store type A, C has highest sales at the end of the year. December months has christmas season. So, that they get highes salary. At the end of the month their sell decrease. We can not find semiler trend for store B and D, it could be there is no\ndata for that time perion. Possible reason is \"store closed\".","metadata":{}},{"cell_type":"markdown","source":"# stationarity of Time Seriese","metadata":{}},{"cell_type":"markdown","source":"Stationarity means that the statistical properties of a time series do not change over time. Some stationary data is (constant mean, constant variance and constant covariance with time). ","metadata":{}},{"cell_type":"markdown","source":"### There are 2 ways to test the stationarity of time series\n* A) Rolling Mean: Visualization \n* B) Dicky - Fuller test: Statistical test","metadata":{}},{"cell_type":"markdown","source":"**A) Rolling Mean:** A rolling analysis of a time series model is often used to assess the model's stability over time. The window is rolled (slid across the data) on a weekly basis, in which the average is taken on a weekly basis. Rolling Statistics is a visualization test, where we can compare the original data with the rolled data and check if the data is stationary or not.\n\n**B) Dicky -Fuller test:** This test provides us the statistical data such as p-value to understand whether we can reject the null hypothesis. If p-value is less than the critical value (say 0.5), we will reject the null hypothesis and say that data is stationary.","metadata":{}},{"cell_type":"code","source":"# lets create a functions to test the stationarity \ndef test_stationarity(timeseries):\n    # Determine rolling statestics \n    roll_mean = timeseries.rolling(window=7).mean()\n    roll_std = timeseries.rolling(window=7).std()\n    \n    # plotting rolling statestics \n    plt.subplots(figsize = (16, 6))\n    orginal = plt.plot(timeseries.resample('w').mean(), color='blue',linewidth= 3, label='Orginal')\n    roll_mean = plt.plot(roll_mean.resample('w').mean(), color='red',linewidth= 3, label='Rolling Mean')\n    roll_mean = plt.plot(roll_std.resample('w').mean(), color='green',linewidth= 3, label='Rolling Std')\n    \n    plt.legend(loc='best')\n    plt.show()\n    \n    # Performing Dickey-Fuller test \n    print('Result of Dickey-Fuller test:')\n    result= adfuller(timeseries, autolag='AIC')\n    \n    print('ADF Statestics: %f' %result[0])\n    print('P-value: %f' %result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print(key, value)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_stationarity(sales_a)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_stationarity(sales_b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_stationarity(sales_c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_stationarity(sales_d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"From above charts we could observe that, mean and variance of the data are not change most over time. So, we do not compute any transformation. ","metadata":{}},{"cell_type":"markdown","source":"# Lets create trends and seasonality ","metadata":{}},{"cell_type":"code","source":"# plotting trends and seasonality \n\ndef plot_timeseries(sales,StoreType):\n\n    fig, axes = plt.subplots(2, 1, sharex=True, sharey=False)\n    fig.set_figheight(6)\n    fig.set_figwidth(20)\n\n    decomposition= seasonal_decompose(sales, model = 'additive',freq=365)\n\n    estimated_trend = decomposition.trend\n    estimated_seasonal = decomposition.seasonal\n    estimated_residual = decomposition.resid\n    \n    axes[1].plot(estimated_seasonal, 'g', label='Seasonality')\n    axes[1].legend(loc='upper left');\n    \n    axes[0].plot(estimated_trend, label='Trend')\n    axes[0].legend(loc='upper left');\n\n    plt.title('Decomposition Plots')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_timeseries(sales_a, 'a')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_timeseries(sales_b, 'b')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_timeseries(sales_c, 'c')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_timeseries(sales_d, 'd')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nFrom the above plots, we can see that there is seasonality and trend present in our data. So, we'll use forecasting models that take both of these factors into consideration. For example, SARIMAX and Prophet.","metadata":{}},{"cell_type":"markdown","source":"# Time Series Forcusting ","metadata":{"trusted":true}},{"cell_type":"markdown","source":"## Evaluation Matrics","metadata":{}},{"cell_type":"markdown","source":"**1. MAE - Mean Absolute Error:** It is the average of the absolute difference between the predicted values and observed values.\n![](https://www.statisticshowto.com/wp-content/uploads/2016/10/MAE.png)\n\n**2. RMSE - Root Mean Square Error:** It is the square root of the average of squared differences between the predicted values and observed values.\n![](https://help.innovyze.com/download/attachments/2459040/scadawatch_analytical_function_rmse_formula.png?version=1&modificationDate=1555033531000&api=v2)","metadata":{}},{"cell_type":"markdown","source":"# Model 01: Seasonal Autoregressive Integrated Moving Average\nIn order to use this model, we need to first find out values of **p, d and q. p** represents number of Autoregressive terms - lags of dependent variable.\n* q represents number of Moving Average terms\n* lagged forecast errors in prediction equation. \n* d represents number of non-seasonal differences.\n\n**To find the values of p, d and q - we use Autocorrelation function (ACF) and Partial Autocorrelation (PACF) plots.**\n\n**ACF** measure of correlation between time series with a lagged version of itself. \n**PACF** measure of correlation between time series with a lagged version of itself but after eliminating the variations already explained by the intervening comparison.\n\n**p value** is the value on x-axis of PACF where the plot crosses the upper Confidence Interval for the first time.\n\n**q value** is the value on x-axis of ACF where the plot crosses the upper Confidence Interval for the first time.\n","metadata":{}},{"cell_type":"markdown","source":"### Autocorrelation function to make ACF and PACF","metadata":{}},{"cell_type":"code","source":"def auto_corr(sales):\n    lag_acf = acf(sales, nlags=30)\n    lag_pacf = pacf(sales,nlags=20,method='ols')\n    \n    plt.subplot(121)\n    plt.plot(lag_acf)\n    plt.axhline(y=0, linestyle='--', color='red')\n    plt.axhline(y=1.96/np.sqrt(len(sales_a)), linestyle='--', color='red')\n    plt.axhline(y=-1.96/np.sqrt(len(sales_a)), linestyle='--', color='red')\n    plt.title('ACF')\n    \n    plt.subplot(122)\n    plt.plot(lag_pacf)\n    plt.axhline(y=0, linestyle='--', color='red')\n    plt.axhline(y=1.96/np.sqrt(len(sales_a)), linestyle='--', color='red')\n    plt.axhline(y=-1.96/np.sqrt(len(sales_a)), linestyle='--', color='red')\n    plt.title('PACF')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ACF and PCF for store A\nauto_corr(sales_a)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ACF and PCF for store B\nauto_corr(sales_b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ACF and PCF for store C\nauto_corr(sales_c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ACF and PCF for store D\nauto_corr(sales_d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graphs suggest that the p = 2 and q = 2 but let's do a grid search and see which combination of p, q and d gives the lowest Akaike information criterion (**AIC**, which tells us the quality of statistical models for a given set of data. Best model uses the lowest number of features to fit the data.\n\nIf we are to predict the sales of each store, we need to consider the whole data set rather than one store of each category. We took one store of each category to understand the tiem series data but from now on, we'll use the whole dataset for modelling","metadata":{}},{"cell_type":"code","source":"# Summering sales on per week basis \n# ARIMA = Autoregresive Integrated Moving Average \n\n\ntrain_arima = train.resample('w').mean()\ntrain_arima = train_arima[['Sales']]\ntrain_arima.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_arima.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparamter turing ARIMA model\nAs discussed above, we have three parameters (p, d and q) for SARIMA model. So, in order to choose the best combination of these parameter, we'll use a grid search. The best combination of parameters will give the lowest AIC score.","metadata":{}},{"cell_type":"code","source":"# Define the p, d and q parameters to take any value between 0 and 3\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA: ')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's iterate through these combinations to see which one gives the lowest AIC score.","metadata":{}},{"cell_type":"code","source":"# Determing p,d,q combinations with AIC scores.\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(train_arima,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we can see that, the above grid search result our optimal paramiter (ARIMA(1, 1, 1)x(1, 1, 1, 12)12 - AIC:1807.3489408440882) ","metadata":{}},{"cell_type":"markdown","source":"### Fitting the model","metadata":{}},{"cell_type":"code","source":"# Fitting the data to SARIMA model \nmodel_sarima = sm.tsa.statespace.SARIMAX(train_arima,\n                                        order=(1, 1, 1),\n                                        seasonal_order=(1,1,1,12),\n                                        enforce_stationarity=False,\n                                        enforce_invertibility=False)\nresults_sarima= model_sarima.fit()\nprint(results_sarima.summary().tables[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking diagnostic plots\nresults_sarima.plot_diagnostics(figsize=(16, 10))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the above 'Histogram plus estimated density' plot that our KDE (Kernel Desnity Estimator) plot closely follows the N(0,1) normal distribution plot. The Normal Q-Q plot shows that the ordered distribution of residuals follows the distribution similar to normal distribution. Thus, our model seems to be pretty good.\n\n**Standardized residual plot tells us that there is no major seasonality trend, which is confirmed by Correlogram (autocorrelation) plot. Autocorrelation plot tells us that the time series residuals have low correlation with lagged versions of itself**","metadata":{}},{"cell_type":"code","source":"# Model prediction \n\npred = results_sarima.get_prediction(start=pd.to_datetime('2015-1-4'), dynamic=False)\n\n# Get confidence interval of forecast \npred_ci = pred.conf_int()\n\nax = train_arima['2014':].plot(label='Observed', figsize=(15,7))\npred.predicted_mean.plot(ax=ax, label='One step ahed Forecast', alpha=1)\n\nax.fill_between(pred_ci.index, \n               pred_ci.iloc[:, 0],\n               pred_ci.iloc[:,1],\n               color='r', alpha=.1)\n\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()\nplt.show()\n\ntrain_arima_forecasted = pred.predicted_mean\ntrain_arima_truth = train_arima['2015-01-04':]\n\nrms_arima= sqrt(mean_squared_error(train_arima_truth,train_arima_forecasted))\nprint('Root Mean Squared Error = ',rms_arima)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save your predicted results for future validation. \n# You could find this results in output sections\n\ntrain_arima_forecasted.to_csv('predicted_data.csv')\nprint('Predicted Data Saved in output')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 2: Prophetic \nFrom our Grid search and foundoptimal parameter we also have another loweset AIC: ARIMA(1, 1, 1)x(0, 1, 1, 12)12 - AIC:1806.29. Lets try to use it ","metadata":{}},{"cell_type":"code","source":"# Creating a train dataset\ntrain_prophet = train.copy()\ntrain_prophet.reset_index(level=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting col names to specific names as required by Prophet library\ntrain_prophet = train_prophet.rename(columns = {'Date': 'ds',\n                                'Sales': 'y'})\ntrain_prophet.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downsampling to week because modelling on daily basis takes a lot of time\nts_week_prophet = train_prophet.set_index(\"ds\").resample(\"W\").sum()\nts_week_prophet.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_store_joined.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MOdel 2: XGBoost\nNow we will drop columns that are correlated (e.g Customers, SalePerCustomer) in addition to merging similar columns into one column (CompetitionOpenSinceMonth, CompetitionOpenSinceYear).","metadata":{}},{"cell_type":"code","source":"# Dropping Customers and Sale per customer\nts_xgboost = train_store_joined.copy()\nts_xgboost = ts_xgboost.drop(['Customers', 'SalesPerCustomer', 'PromoInterval'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ts_xgboost.head()\n# Here we do not have any categorical variables so we do not have to convert them into numerical to use in XGBoost ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining similar columns into one column and dropping old columns\nts_xgboost['CompetitionOpen'] = 12 * (ts_xgboost.Year - ts_xgboost.CompetitionOpenSinceYear) + (ts_xgboost.Month - ts_xgboost.CompetitionOpenSinceMonth)\nts_xgboost['PromoOpen'] = 12 * (ts_xgboost.Year - ts_xgboost.Promo2SinceYear) + (ts_xgboost.WeekofYear - ts_xgboost.Promo2SinceWeek) / 4.0\nts_xgboost = ts_xgboost.drop([\"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\"], axis = 1)\nts_xgboost = ts_xgboost.drop([\"Promo2SinceWeek\", \"Promo2SinceYear\"], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting categorical cols to numerical cols and removing old cols\nmappings = {0:0, \"0\": 0, \"a\": 1, \"b\": 1, \"c\": 1}\nts_xgboost[\"StateHoliday_cat\"] = ts_xgboost[\"StateHoliday\"].map(mappings)\nts_xgboost[\"StoreType_cat\"] = ts_xgboost[\"StoreType\"].map(mappings)\nts_xgboost[\"Assortment_cat\"] = ts_xgboost[\"Assortment\"].map(mappings)\nts_xgboost = ts_xgboost.drop([\"StateHoliday\", \"StoreType\", \"Assortment\"], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data\nfeatures = ts_xgboost.drop([\"Sales\"], axis = 1)\ntarget = ts_xgboost[\"Sales\"]\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(features, target, test_size = 0.20) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline XGBoost ","metadata":{}},{"cell_type":"code","source":"# Tuning parameters - using default metrics\nparams = {'max_depth':6, \"booster\": \"gbtree\", 'eta':0.3, 'objective':'reg:linear'} \n\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_test, y_test)\nwatchlist = [(dtrain, 'train'), (dtest, 'eval')]\n\n# Training the model\nxgboost = xgb.train(params, dtrain, 100, evals=watchlist,early_stopping_rounds= 100, verbose_eval=True)\n         \n# Making predictions\npreds = xgboost.predict(dtest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSE of model\nrms_xgboost = sqrt(mean_squared_error(y_test, preds))\nprint(\"Root Mean Squared Error for XGBoost:\", rms_xgboost)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hypertuning XGBoost\nNow let's try to decrease the RMSE of XGBoost by passing different values for our hyperparameters in the XGBoost model.\n\n**eta:** It defines the learning rate i.e step size to learn the data in the gradient descent modeling (the basis for XGBoost). The default value is 0.3 but we want to keep the learning rate low to avoid overfitting. So, we'll choose **0.2** as eta\n\n**max_depth:** Maximum depth of a tree. The default value is 6 but we want our model to be more complex and find good predictions. So, let's choose 10 as max depth.\n\n**gamma:** Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be. The default value is 0, let's choose a little higher value so as to get good predictions","metadata":{}},{"cell_type":"code","source":"# Tuning parameters\nparams_2 = {'max_depth':10, 'eta':0.1,  'gamma': 2}\n\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_test, y_test)\nwatchlist = [(dtrain, 'train'), (dtest, 'eval')]\n\n# Training the model\nxgboost_2 = xgb.train(params_2, dtrain, 100, evals=watchlist,early_stopping_rounds= 100, verbose_eval=True)\n         \n# Making predictions\npreds_2 = xgboost_2.predict(dtest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RMSE of model\nrms_xgboost_2 = sqrt(mean_squared_error(y_test, preds_2))\nprint(\"Root Mean Squared Error for XGBoost:\", rms_xgboost_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the feature importance\nfig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(xgboost_2, max_num_features=50, height=0.8, ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final XGBoost Model:\nAfter hypertuning, we were able to reduce RMSE from 1223.31 to 1176.20 which is great! Now, let's compare the performance of all models","metadata":{}},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"# Comparing performance of above three models - through RMSE\nrms_arima = format(float(rms_arima))\nrms_xgboost_2 = format(float(rms_xgboost_2))\n\nmodel_errors = pd.DataFrame({\n    \"Model\": [\"SARIMA\",  \"XGBoost\"],\n    \"RMSE\": [rms_arima, rms_xgboost_2]\n})\n\nmodel_errors.sort_values(by = \"RMSE\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparison & Selection\nWe used the Root Mean Squared Error **(RMSE)** to evaluate and validate the performance of various models used. Let's see which model performed better and why/why not.\n\na) We can see from the above table that **SARIMA** performs the best than **XGBoost**.\n\nb) It makes sense because **SARIMA is designed specifically for seasonal time series data while XGBoost is a general (though powerful) machine learning approach with various applications.**\n\n\nBased on the above analysis, we'll choose ARIMA as our final model to predict the sales because it gives us the least RMSE and is well suited to our needs of predicting time series seasonal data.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}